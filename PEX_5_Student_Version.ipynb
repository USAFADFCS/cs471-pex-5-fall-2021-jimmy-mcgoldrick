{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "PEX_5_Student_Version.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KBl_Syo5Fc5"
      },
      "source": [
        "### C1C Jimmy McGoldrick\n",
        "### CS 471\n",
        "### PEX 5\n",
        "### 8 Dec 2021\n",
        "### Documentation Statement: I used no unauthorized resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNElZJ2ZOEHQ"
      },
      "source": [
        "## 80 Points\n",
        "\n",
        "In PEX 5 you will train the YOLO object detection algorihtm to detect objects that interest you. If you are not feeling creative, you can collect data and train a model to detect playing card suits and face values. You will then do something interesting with your model. For example, with the playing card data set, you can use the web cam to play blackjack without user input. \n",
        "\n",
        "The graded objectives for PEX 5 are:\n",
        "\n",
        "(10 pts) Collect and label data for use in building a custom YOLO model\n",
        "\n",
        "(20 pts) Train the YOLO model to high accuracy for your domain\n",
        "\n",
        "  **\"Lt Col Maher, what is high accuracy?\" ... It depends on your domain. If you want to make sure you recieve full credit, write down all of your efforts to improve the accuracy of the model, and write a statement why you think you have achieved peak accuracy in this model. Charts and graphs will help to justify your stance. \n",
        "\n",
        "(10 pts) Use data analysis to interpret the Mean Average Precision, accuracy and recall of your model\n",
        "\n",
        "(20 pts) Enable object detection through a web camera\n",
        "\n",
        "(20 pts) Write code that does something interesting with your model. \n",
        "\n",
        "(10 pts) Write a 300-500 word essay describing the possible ethical implications of your project. Reference an ethical framework to justify your view. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFLMd6hqQ3q_"
      },
      "source": [
        "**AUTHORIZED RESOURCES:** Any material from the CS 471 course site and online resources. You may reuse online code as long as you describe what the code is doing in your comments and you modify it to solve this problem. Don't forget to document any online code sources. \n",
        "\n",
        "**NOTE:**\n",
        "\n",
        "*  Never copy another person’s or group’s work and submit it as your own.\n",
        "*  Do not jointly create a program or complete this assignment unless explicitly allowed.\n",
        "*   You must document all help received from sources other than your instructor or instructor-provided course materials (including your textbook).\n",
        "\n",
        "**Documentation Policy:**\n",
        "\n",
        "*   You must document all help received from any source other than your instructor or instructor-provided materials, including your textbook (unless directly quoting or paraphrasing).\n",
        "*   The documentation statement must explicitly describe WHAT assistance was provided, WHERE on the assignment the assistance was provided, and WHO provided the assistance, and HOW it was used in completing the assignment.\n",
        "*   If no help was received on this assignment, the documentation statement must state “None.”\n",
        "*   If you checked answers with anyone, you must document with whom on which problems. You must document whether or not you made any changes, and if you did make changes you must document the problems you changed and the reasons why.\n",
        "*   Vague documentation statements must be corrected before the assignment will be graded and will result in a 5% deduction on the assignment.\n",
        "\n",
        "**Turn-in Policies:**\n",
        "\n",
        "*   On-time turn-in is at the specific day and time listed above.\n",
        "*   Post the required solution files to your Github Classroom repo.\n",
        "*   Only 1 turn-in required per team.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6p6HAuM4HGM"
      },
      "source": [
        "## (10 pts) Task 1 Train and Label data for use in building a custom YOLO model. \n",
        "I recommend creating an account with Roboflow.com to upload pictures and label the data set. There are several other tools available that you are welcome to use. If you use another tool, I will need a way to access your data; please provide that method in the text box below. If you use Roboflow, copy and paste the link to your Roboflow project page below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LQuompXhi7q"
      },
      "source": [
        "I chose to use roboflow to focus on detecting different types of cars within classic highway scenes, because this to me is a classic computer vision problem that has a lot of applicability with self-driving cars becoming more and more popular. Link to roboflow page: https://app.roboflow.com/ds/imUBuOO6pW?key=XEddUMrGCB"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKRqdZzf4xOx"
      },
      "source": [
        "## (20 pts) Task 2 Train a Yolo model to high accuracy. \n",
        "\n",
        "Keep a log of your experimentation with improving your model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG8KuGBPkrF8"
      },
      "source": [
        "Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AWxWgNLwksUM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3cf8d56c-eb0c-42eb-98c2-946e19b3044d"
      },
      "source": [
        "# clone YOLOv5 repository\n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "!git reset --hard 886f1c03d839575afecb059accf74296fad395b6"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 10008, done.\u001b[K\n",
            "remote: Total 10008 (delta 0), reused 0 (delta 0), pack-reused 10008\u001b[K\n",
            "Receiving objects: 100% (10008/10008), 10.34 MiB | 27.22 MiB/s, done.\n",
            "Resolving deltas: 100% (6940/6940), done.\n",
            "/content/yolov5\n",
            "HEAD is now at 886f1c0 DDP after autoanchor reorder (#2421)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXTPCt0g5AHo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a99a6174-8d52-4f3c-f225-65e41bcd4a64"
      },
      "source": [
        "# install dependencies as necessary\n",
        "!pip install -qr requirements.txt  # install dependencies (ignore errors)\n",
        "import torch\n",
        "\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "from utils.google_utils import gdrive_download  # to download models/datasets\n",
        "\n",
        "# clear_output()\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Setup complete. Using torch 1.10.0+cu111 CPU\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCHimvv2k3Ez"
      },
      "source": [
        "Download Dataset from Roboflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E91utO4jk54J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a4f32110-74ad-4bf9-b194-674908eb3e41"
      },
      "source": [
        "!pip install roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "rf = Roboflow(api_key=\"L9xEJjvMffR584Ifmbzo\")\n",
        "project = rf.workspace().project(\"pex4\")\n",
        "dataset = project.version(2).download(\"yolov5\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: roboflow in /usr/local/lib/python3.7/dist-packages (0.1.8)\n",
            "Requirement already satisfied: Pillow>=7.1.2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (7.1.2)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.8.2)\n",
            "Requirement already satisfied: kiwisolver==1.3.1 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.3.1)\n",
            "Requirement already satisfied: chardet==4.0.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.0.0)\n",
            "Requirement already satisfied: idna==2.10 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.10)\n",
            "Requirement already satisfied: numpy>=1.18.5 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.19.5)\n",
            "Requirement already satisfied: certifi==2021.5.30 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2021.5.30)\n",
            "Requirement already satisfied: wget in /usr/local/lib/python3.7/dist-packages (from roboflow) (3.2)\n",
            "Requirement already satisfied: python-dotenv in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.19.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.15.0)\n",
            "Requirement already satisfied: tqdm>=4.41.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.62.3)\n",
            "Requirement already satisfied: cycler==0.10.0 in /usr/local/lib/python3.7/dist-packages (from roboflow) (0.10.0)\n",
            "Requirement already satisfied: pyparsing==2.4.7 in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.4.7)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (from roboflow) (3.2.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from roboflow) (2.26.0)\n",
            "Requirement already satisfied: opencv-python>=4.1.2 in /usr/local/lib/python3.7/dist-packages (from roboflow) (4.1.2.30)\n",
            "Requirement already satisfied: PyYAML>=5.3.1 in /usr/local/lib/python3.7/dist-packages (from roboflow) (6.0)\n",
            "Requirement already satisfied: urllib3==1.26.6 in /usr/local/lib/python3.7/dist-packages (from roboflow) (1.26.6)\n",
            "Requirement already satisfied: charset-normalizer~=2.0.0 in /usr/local/lib/python3.7/dist-packages (from requests->roboflow) (2.0.7)\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in PEX4-2 to yolov5pytorch: 100% [1972262 / 1972262] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to PEX4-2 in yolov5pytorch:: 100%|██████████| 156/156 [00:00<00:00, 1309.21it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBzrWcVnlMjm"
      },
      "source": [
        "Download Data.yml File"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "o2HCD1_UlQe0"
      },
      "source": [
        "# this is the YAML file Roboflow wrote for us that we're loading into this notebook with our data\n",
        "#%cat /content/yolov5/data.yaml\n",
        "dataset.location = '/content/yolov5'"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYo2TrkflFUm"
      },
      "source": [
        "Define Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Be4-JVN9lGdu",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 227
        },
        "outputId": "5ab46cbc-bc40-4179-d580-9ae4e5e54cb8"
      },
      "source": [
        "# define number of classes based on YAML\n",
        "import yaml\n",
        "with open(dataset.location + \"/data.yaml\", 'r') as stream:\n",
        "    num_classes = str(yaml.safe_load(stream)['nc'])"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-7-cf0e194a3a0c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# define number of classes based on YAML\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0myaml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlocation\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m\"/data.yaml\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mstream\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mnum_classes\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myaml\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msafe_load\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstream\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'nc'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '/content/yolov5/data.yaml'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pyib1REjlasX",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "da6ac626-49c7-4d2d-d78c-b31295c280ce"
      },
      "source": [
        "#this is the model configuration we will use for our tutorial \n",
        "%cat /content/yolov5/models/yolov5s.yaml"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "# parameters\n",
            "nc: 80  # number of classes\n",
            "depth_multiple: 0.33  # model depth multiple\n",
            "width_multiple: 0.50  # layer channel multiple\n",
            "\n",
            "# anchors\n",
            "anchors:\n",
            "  - [10,13, 16,30, 33,23]  # P3/8\n",
            "  - [30,61, 62,45, 59,119]  # P4/16\n",
            "  - [116,90, 156,198, 373,326]  # P5/32\n",
            "\n",
            "# YOLOv5 backbone\n",
            "backbone:\n",
            "  # [from, number, module, args]\n",
            "  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n",
            "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
            "   [-1, 3, C3, [128]],\n",
            "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
            "   [-1, 9, C3, [256]],\n",
            "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
            "   [-1, 9, C3, [512]],\n",
            "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
            "   [-1, 1, SPP, [1024, [5, 9, 13]]],\n",
            "   [-1, 3, C3, [1024, False]],  # 9\n",
            "  ]\n",
            "\n",
            "# YOLOv5 head\n",
            "head:\n",
            "  [[-1, 1, Conv, [512, 1, 1]],\n",
            "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
            "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
            "   [-1, 3, C3, [512, False]],  # 13\n",
            "\n",
            "   [-1, 1, Conv, [256, 1, 1]],\n",
            "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
            "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
            "   [-1, 3, C3, [256, False]],  # 17 (P3/8-small)\n",
            "\n",
            "   [-1, 1, Conv, [256, 3, 2]],\n",
            "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
            "   [-1, 3, C3, [512, False]],  # 20 (P4/16-medium)\n",
            "\n",
            "   [-1, 1, Conv, [512, 3, 2]],\n",
            "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
            "   [-1, 3, C3, [1024, False]],  # 23 (P5/32-large)\n",
            "\n",
            "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
            "  ]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZRZkb405lfQu"
      },
      "source": [
        "#customize iPython writefile so we can write variables\n",
        "from IPython.core.magic import register_line_cell_magic\n",
        "\n",
        "@register_line_cell_magic\n",
        "def writetemplate(line, cell):\n",
        "    with open(line, 'w') as f:\n",
        "        f.write(cell.format(**globals()))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKKbj81LliXi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 398
        },
        "outputId": "e82fab46-27b5-4f7d-c23b-39c18604b209"
      },
      "source": [
        "%%writetemplate /content/yolov5/models/custom_yolov5s.yaml\n",
        "\n",
        "# parameters\n",
        "nc: {num_classes}  # number of classes\n",
        "depth_multiple: 0.33  # model depth multiple\n",
        "width_multiple: 0.50  # layer channel multiple\n",
        "\n",
        "# anchors\n",
        "anchors:\n",
        "  - [10,13, 16,30, 33,23]  # P3/8\n",
        "  - [30,61, 62,45, 59,119]  # P4/16\n",
        "  - [116,90, 156,198, 373,326]  # P5/32\n",
        "\n",
        "# YOLOv5 backbone\n",
        "backbone:\n",
        "  # [from, number, module, args]\n",
        "  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n",
        "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
        "   [-1, 3, BottleneckCSP, [128]],\n",
        "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
        "   [-1, 9, BottleneckCSP, [256]],\n",
        "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
        "   [-1, 9, BottleneckCSP, [512]],\n",
        "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
        "   [-1, 1, SPP, [1024, [5, 9, 13]]],\n",
        "   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n",
        "  ]\n",
        "\n",
        "# YOLOv5 head\n",
        "head:\n",
        "  [[-1, 1, Conv, [512, 1, 1]],\n",
        "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
        "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
        "   [-1, 3, BottleneckCSP, [512, False]],  # 13\n",
        "\n",
        "   [-1, 1, Conv, [256, 1, 1]],\n",
        "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
        "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
        "   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n",
        "\n",
        "   [-1, 1, Conv, [256, 3, 2]],\n",
        "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
        "   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n",
        "\n",
        "   [-1, 1, Conv, [512, 3, 2]],\n",
        "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
        "   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n",
        "\n",
        "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
        "  ]"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-10-a8a1e5511490>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mget_ipython\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_cell_magic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'writetemplate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'/content/yolov5/models/custom_yolov5s.yaml'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"\\n# parameters\\nnc: {num_classes}  # number of classes\\ndepth_multiple: 0.33  # model depth multiple\\nwidth_multiple: 0.50  # layer channel multiple\\n\\n# anchors\\nanchors:\\n  - [10,13, 16,30, 33,23]  # P3/8\\n  - [30,61, 62,45, 59,119]  # P4/16\\n  - [116,90, 156,198, 373,326]  # P5/32\\n\\n# YOLOv5 backbone\\nbackbone:\\n  # [from, number, module, args]\\n  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\\n   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\\n   [-1, 3, BottleneckCSP, [128]],\\n   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\\n   [-1, 9, BottleneckCSP, [256]],\\n   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\\n   [-1, 9, BottleneckCSP, [512]],\\n   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\\n   [-1, 1, SPP, [1024, [5, 9, 13]]],\\n   [-1, 3, BottleneckCSP, [1024, False]],  # 9\\n  ]\\n\\n# YOLOv5 head\\nhead:\\n  [[-1, 1, Conv, [512, 1, 1]],\\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\\n   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\\n   [-1, 3, BottleneckCSP, [512, False]],  # 13\\n\\n   [-1, 1, Conv, [256, 1, 1]],\\n   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\\n   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\\n   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\\n\\n   [-1, 1, Conv, [256, 3, 2]],\\n   [[-1, 14], 1, Concat, [1]],  # cat head P4\\n   [-1, 3, BottleneckCSP, [512, False...\n\u001b[0m",
            "\u001b[0;32m/usr/local/lib/python3.7/dist-packages/IPython/core/interactiveshell.py\u001b[0m in \u001b[0;36mrun_cell_magic\u001b[0;34m(self, magic_name, line, cell)\u001b[0m\n\u001b[1;32m   2115\u001b[0m             \u001b[0mmagic_arg_s\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvar_expand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstack_depth\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2116\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuiltin_trap\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2117\u001b[0;31m                 \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmagic_arg_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2118\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2119\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-9-fafe4f3ea491>\u001b[0m in \u001b[0;36mwritetemplate\u001b[0;34m(line, cell)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mwritetemplate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcell\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mline\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'w'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwrite\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcell\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m**\u001b[0m\u001b[0mglobals\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m: 'num_classes'"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21Hrf0G-qx4T"
      },
      "source": [
        "Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bff0hKcXqzBv"
      },
      "source": [
        "# train yolov5s on custom data for 100 epochs\n",
        "# time its performance\n",
        "%%time\n",
        "%cd /content/yolov5/\n",
        "!python train.py --img 416 --batch 16 --epochs 2 --data {dataset.location}/data.yaml --cfg ./models/custom_yolov5s.yaml --weights ' ' --name yolov5s_results  --cache --exist-ok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxxqTl5Lr0xq"
      },
      "source": [
        "Run Inference Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgyxW9n8r6n0"
      },
      "source": [
        "# when we ran this, we saw .007 second inference time. That is 140 FPS on a TESLA P100!\n",
        "# use the best weights!\n",
        "%cd /content/yolov5/\n",
        "!python detect.py --weights /content/yolov5/runs/train/yolov5s_results/weights/best.pt --img 416 --conf 0.2 --source YOLOv5-Lab-Day-Test-1/test/images"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ux9TPNdorzD_"
      },
      "source": [
        "Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jzK2gmwqr3Cl"
      },
      "source": [
        "#display inference on ALL test images\n",
        "#this looks much better with longer training above\n",
        "\n",
        "import glob\n",
        "from IPython.display import Image, display\n",
        "\n",
        "for imageName in glob.glob('/content/yolov5/runs/detect/exp4/*.jpg'): #assuming JPG\n",
        "    display(Image(filename=imageName))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l_w2hRYjsYUu"
      },
      "source": [
        "Export Weights for Future Reference"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OXhFS-vPsaQi"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "%cp /content/yolov5/runs/train/yolov5s_results/weights/best.pt /content/gdrive/My\\ Drive"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv40JgCq5WRI"
      },
      "source": [
        "## (20 pts) Task 3 Justify how well your model performs\n",
        "Provide charts showing at least the Mean Average Precision, Accuracy, and Recall of your model. Discuss the charts and what these results mean. You may want to include a discussion on overfitting and underfitting in your discussion. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0kyfGGCq5S5"
      },
      "source": [
        "Evaluate the Model with Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBi5Qy_n5m3L"
      },
      "source": [
        "# Start tensorboard\n",
        "# Launch after you have started training\n",
        "# logs save in the folder \"runs\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KsJvHL0trJnS"
      },
      "source": [
        "Evaluate the Model with old-school Graphs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lfA8H9GXrMOT"
      },
      "source": [
        "# we can also output some older school graphs if the tensor board isn't working for whatever reason... \n",
        "from utils.plots import plot_results  # plot results.txt as results.png\n",
        "Image(filename='/content/yolov5/runs/train/yolov5s_results6/results.png', width=1000)  # view results.png"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ALVFGJiXrZRf"
      },
      "source": [
        "Evaluate the Model with Training Images"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1T5QHYkSrl50"
      },
      "source": [
        "# first, display our ground truth data\n",
        "print(\"GROUND TRUTH TRAINING DATA:\")\n",
        "Image(filename='/content/yolov5/runs/train/yolov5s_results6/test_batch0_labels.jpg', width=900)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzJlZA7v5sR3"
      },
      "source": [
        "### Answer to Task 3. Add any discussion of your models to this box. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys-bw08N5xVj"
      },
      "source": [
        "## (20 pts) Task 4. Connect your model to Webcam streaming\n",
        "\n",
        "Enable your model to predict in real-time on a web camera, using your custom model. If the objects you train your model to are not something commonly available, please provide the web cam code and a pre-recorded video of you detecting the object. \n",
        "\n",
        "Hint: On the left hand side, Google Colab gives you code for accessing your webcam with Javascript. Use this code to get your webcam working. YOLO has a webcam input functionality built-in, but you cannot use this functionality, because it is attempting to open the webcam on Google's server and not your laptop. To execute this step\n",
        "\n",
        "1.   Modify the webcam code so that it will open the webcam,\n",
        "2.   take a picture,\n",
        "1.   close the webcam,\n",
        "2.   run the detection algorithm,\n",
        "1.   display the detection image\n",
        "2.   and then repeat 5 times.\n",
        "\n",
        "This will not be the same as real-time webcam footage, but it will get you as close as you can get on Google Colab. \n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gVSuo4C4524j"
      },
      "source": [
        "##Answer to Task 4. Paste your code here. You may use code from the internet and in-class exercises "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypCp6QEX6IB4"
      },
      "source": [
        "### Answer to Task 4 (Optional) Add pre-recorded video, if necessary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfzjxmhy7jbI"
      },
      "source": [
        "## (10 pts) Task 5. Create something interesting using your model\n",
        "\n",
        "You have a limited time, so don't make this a huge feature, just something cool your model could do. For example, something that counts the objects coming across the webcam would receive full points. If you have a more creative idea, I may be inclined to add some bonus points, but make sure you are taking care of your other classwork as well. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TkgMmCbibgah"
      },
      "source": [
        "#Answer to Task 5."
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvelYJcI8Ove"
      },
      "source": [
        "## (10 pts) Task 6. Write 300-500 words on the ethical implications of your project.\n",
        "Make sure you support your thoughts with ethical frameworks from ACM, IEEE, or any other reputible source. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktf121Zv8c8c"
      },
      "source": [
        "### Answer to Task 6"
      ]
    }
  ]
}