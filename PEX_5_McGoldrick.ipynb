{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "PEX_5_McGoldrick.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "wNElZJ2ZOEHQ",
        "DG8KuGBPkrF8",
        "DCHimvv2k3Ez",
        "rKRqdZzf4xOx",
        "MYo2TrkflFUm",
        "21Hrf0G-qx4T",
        "HmbxbKGgeu_7",
        "vxxqTl5Lr0xq",
        "6n4JUSlKMVye",
        "a0kyfGGCq5S5",
        "b9PK0ROFlZHB",
        "KvelYJcI8Ove"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.8"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4KBl_Syo5Fc5"
      },
      "source": [
        "### C1C Jimmy McGoldrick\n",
        "### CS 471\n",
        "### PEX 5\n",
        "### 8 Dec 2021\n",
        "### Documentation Statement: I used no unauthorized resources."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wNElZJ2ZOEHQ"
      },
      "source": [
        "## 80 Points\n",
        "\n",
        "In PEX 5 you will train the YOLO object detection algorihtm to detect objects that interest you. If you are not feeling creative, you can collect data and train a model to detect playing card suits and face values. You will then do something interesting with your model. For example, with the playing card data set, you can use the web cam to play blackjack without user input. \n",
        "\n",
        "The graded objectives for PEX 5 are:\n",
        "\n",
        "(10 pts) Collect and label data for use in building a custom YOLO model\n",
        "\n",
        "(20 pts) Train the YOLO model to high accuracy for your domain\n",
        "\n",
        "  **\"Lt Col Maher, what is high accuracy?\" ... It depends on your domain. If you want to make sure you recieve full credit, write down all of your efforts to improve the accuracy of the model, and write a statement why you think you have achieved peak accuracy in this model. Charts and graphs will help to justify your stance. \n",
        "\n",
        "(10 pts) Use data analysis to interpret the Mean Average Precision, accuracy and recall of your model\n",
        "\n",
        "(20 pts) Enable object detection through a web camera\n",
        "\n",
        "(20 pts) Write code that does something interesting with your model. \n",
        "\n",
        "(10 pts) Write a 300-500 word essay describing the possible ethical implications of your project. Reference an ethical framework to justify your view. \n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uFLMd6hqQ3q_"
      },
      "source": [
        "**AUTHORIZED RESOURCES:** Any material from the CS 471 course site and online resources. You may reuse online code as long as you describe what the code is doing in your comments and you modify it to solve this problem. Don't forget to document any online code sources. \n",
        "\n",
        "**NOTE:**\n",
        "\n",
        "*  Never copy another person’s or group’s work and submit it as your own.\n",
        "*  Do not jointly create a program or complete this assignment unless explicitly allowed.\n",
        "*   You must document all help received from sources other than your instructor or instructor-provided course materials (including your textbook).\n",
        "\n",
        "**Documentation Policy:**\n",
        "\n",
        "*   You must document all help received from any source other than your instructor or instructor-provided materials, including your textbook (unless directly quoting or paraphrasing).\n",
        "*   The documentation statement must explicitly describe WHAT assistance was provided, WHERE on the assignment the assistance was provided, and WHO provided the assistance, and HOW it was used in completing the assignment.\n",
        "*   If no help was received on this assignment, the documentation statement must state “None.”\n",
        "*   If you checked answers with anyone, you must document with whom on which problems. You must document whether or not you made any changes, and if you did make changes you must document the problems you changed and the reasons why.\n",
        "*   Vague documentation statements must be corrected before the assignment will be graded and will result in a 5% deduction on the assignment.\n",
        "\n",
        "**Turn-in Policies:**\n",
        "\n",
        "*   On-time turn-in is at the specific day and time listed above.\n",
        "*   Post the required solution files to your Github Classroom repo.\n",
        "*   Only 1 turn-in required per team.\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b6p6HAuM4HGM"
      },
      "source": [
        "## (10 pts) Task 1 Train and Label data for use in building a custom YOLO model. \n",
        "I recommend creating an account with Roboflow.com to upload pictures and label the data set. There are several other tools available that you are welcome to use. If you use another tool, I will need a way to access your data; please provide that method in the text box below. If you use Roboflow, copy and paste the link to your Roboflow project page below."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4LQuompXhi7q"
      },
      "source": [
        "### I chose to use roboflow to upload and annotate an ASL dictionary, because I think it would be interesting to see computer vision used to create vision-to-speech in the same way that text-to-speech is used to help visually impaired. https://app.roboflow.com/c22james-mcgoldrick-usafa-edu/asl-computer-vision-library"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cx9Wb2zP1EyY"
      },
      "source": [
        "### After creating and annotating my own data set, I found a publicly available ASL object detection library that contains over 750 images -- way more than mine. I used this one to train my model."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DG8KuGBPkrF8"
      },
      "source": [
        "### Install Dependencies"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AWxWgNLwksUM",
        "outputId": "b21fa4d2-85dd-42f9-b832-9fd8c1f5298d"
      },
      "source": [
        "# clone YOLOv5 repository\n",
        "!git clone https://github.com/ultralytics/yolov5  # clone repo\n",
        "%cd yolov5\n",
        "!git reset --hard 886f1c03d839575afecb059accf74296fad395b6\n",
        "# install dependencies as necessary\n",
        "\n",
        "import torch\n",
        "import yaml\n",
        "import glob\n",
        "\n",
        "!pip install -qr requirements.txt  # install dependencies (ignore errors)\n",
        "!pip install -q roboflow\n",
        "\n",
        "from roboflow import Roboflow\n",
        "from IPython.display import Image, display\n",
        "from IPython.core.magic import register_line_cell_magic\n",
        "from IPython.display import Image, clear_output  # to display images\n",
        "from IPython.display import display, Javascript\n",
        "from google.colab.output import eval_js\n",
        "from base64 import b64decode\n",
        "from pathlib import Path\n",
        "from utils.google_utils import gdrive_download  # to download models/datasets\n",
        "\n",
        "# clear_output()\n",
        "print('Setup complete. Using torch %s %s' % (torch.__version__, torch.cuda.get_device_properties(0) if torch.cuda.is_available() else 'CPU'))"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'yolov5'...\n",
            "remote: Enumerating objects: 10187, done.\u001b[K\n",
            "remote: Counting objects: 100% (27/27), done.\u001b[K\n",
            "remote: Compressing objects: 100% (14/14), done.\u001b[K\n",
            "remote: Total 10187 (delta 15), reused 25 (delta 13), pack-reused 10160\u001b[K\n",
            "Receiving objects: 100% (10187/10187), 10.43 MiB | 23.48 MiB/s, done.\n",
            "Resolving deltas: 100% (7061/7061), done.\n",
            "/content/yolov5/yolov5\n",
            "HEAD is now at 886f1c0 DDP after autoanchor reorder (#2421)\n",
            "Setup complete. Using torch 1.10.0+cu111 _CudaDeviceProperties(name='Tesla K80', major=3, minor=7, total_memory=11441MB, multi_processor_count=13)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DCHimvv2k3Ez"
      },
      "source": [
        "### Download Dataset from Roboflow"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KzyXRlhfthFo",
        "outputId": "446294ea-fafd-45cc-e503-3f8b50c7b54f"
      },
      "source": [
        "#follow the link below to get your download code from from Roboflow\n",
        "rf = Roboflow(model_format=\"yolov5\", notebook=\"roboflow-yolov5\")\n",
        "%cd /content/yolov5\n",
        "rf = Roboflow(api_key=\"L9xEJjvMffR584Ifmbzo\")\n",
        "#project = rf.workspace().project(\"asl-computer-vision-library\")\n",
        "project = rf.workspace().project(\"american-sign-language-letters-hngay\")\n",
        "dataset = project.version(1).download(\"yolov5\")\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "upload and label your dataset, and get an API KEY here: https://app.roboflow.com/?model=yolov5&ref=roboflow-yolov5\n",
            "/content/yolov5\n",
            "loading Roboflow workspace...\n",
            "loading Roboflow project...\n",
            "Downloading Dataset Version Zip in American-Sign-Language-Letters-1 to yolov5pytorch: 100% [11115655 / 11115655] bytes\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Extracting Dataset Version Zip to American-Sign-Language-Letters-1 in yolov5pytorch:: 100%|██████████| 1451/1451 [00:01<00:00, 1201.05it/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rKRqdZzf4xOx"
      },
      "source": [
        "## (20 pts) Task 2 Train a Yolo model to high accuracy. \n",
        "\n",
        "Keep a log of your experimentation with improving your model. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MYo2TrkflFUm"
      },
      "source": [
        "### Define the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Be4-JVN9lGdu",
        "outputId": "e49eae13-1e3d-46be-f950-b5545de3a1e5"
      },
      "source": [
        "# define number of classes based on YAML\n",
        "with open(dataset.location + \"/data.yaml\", 'r') as stream: num_classes = str(yaml.safe_load(stream)['nc'])\n",
        "print(num_classes + \" potential detection classes\")\n",
        "\n",
        "#customize iPython writefile so we can write variables\n",
        "@register_line_cell_magic\n",
        "def writetemplate(line, cell):\n",
        "    with open(line, 'w') as f:\n",
        "        f.write(cell.format(**globals()))"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "26 potential detection classes\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YjEqd9wqk-G6"
      },
      "source": [
        "#### Write out a template to base the model from"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qKKbj81LliXi"
      },
      "source": [
        "%%writetemplate /content/yolov5/models/custom_yolov5s.yaml\n",
        "\n",
        "# parameters\n",
        "nc: {num_classes}  # number of classes\n",
        "depth_multiple: 0.33  # model depth multiple\n",
        "width_multiple: 0.50  # layer channel multiple\n",
        "\n",
        "# anchors\n",
        "anchors:\n",
        "  - [10,13, 16,30, 33,23]  # P3/8\n",
        "  - [30,61, 62,45, 59,119]  # P4/16\n",
        "  - [116,90, 156,198, 373,326]  # P5/32\n",
        "\n",
        "# YOLOv5 backbone\n",
        "backbone:\n",
        "  # [from, number, module, args]\n",
        "  [[-1, 1, Focus, [64, 3]],  # 0-P1/2\n",
        "   [-1, 1, Conv, [128, 3, 2]],  # 1-P2/4\n",
        "   [-1, 3, BottleneckCSP, [128]],\n",
        "   [-1, 1, Conv, [256, 3, 2]],  # 3-P3/8\n",
        "   [-1, 9, BottleneckCSP, [256]],\n",
        "   [-1, 1, Conv, [512, 3, 2]],  # 5-P4/16\n",
        "   [-1, 9, BottleneckCSP, [512]],\n",
        "   [-1, 1, Conv, [1024, 3, 2]],  # 7-P5/32\n",
        "   [-1, 1, SPP, [1024, [5, 9, 13]]],\n",
        "   [-1, 3, BottleneckCSP, [1024, False]],  # 9\n",
        "  ]\n",
        "\n",
        "# YOLOv5 head\n",
        "head:\n",
        "  [[-1, 1, Conv, [512, 1, 1]],\n",
        "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
        "   [[-1, 6], 1, Concat, [1]],  # cat backbone P4\n",
        "   [-1, 3, BottleneckCSP, [512, False]],  # 13\n",
        "\n",
        "   [-1, 1, Conv, [256, 1, 1]],\n",
        "   [-1, 1, nn.Upsample, [None, 2, 'nearest']],\n",
        "   [[-1, 4], 1, Concat, [1]],  # cat backbone P3\n",
        "   [-1, 3, BottleneckCSP, [256, False]],  # 17 (P3/8-small)\n",
        "\n",
        "   [-1, 1, Conv, [256, 3, 2]],\n",
        "   [[-1, 14], 1, Concat, [1]],  # cat head P4\n",
        "   [-1, 3, BottleneckCSP, [512, False]],  # 20 (P4/16-medium)\n",
        "\n",
        "   [-1, 1, Conv, [512, 3, 2]],\n",
        "   [[-1, 10], 1, Concat, [1]],  # cat head P5\n",
        "   [-1, 3, BottleneckCSP, [1024, False]],  # 23 (P5/32-large)\n",
        "\n",
        "   [[17, 20, 23], 1, Detect, [nc, anchors]],  # Detect(P3, P4, P5)\n",
        "  ]"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21Hrf0G-qx4T"
      },
      "source": [
        "### Train the Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Bff0hKcXqzBv"
      },
      "source": [
        "# train yolov5s on custom data\n",
        "# time its performance\n",
        "%%time\n",
        "%cd /content/yolov5/\n",
        "!python train.py --img 416 --batch 8 --epochs 50 --data {dataset.location}/data.yaml --cfg ./models/custom_yolov5s.yaml --weights /content/yolov5/weights/best.pt --name yolov5s_results  --cache --exist-ok\n",
        "\n",
        "#save weights\n",
        "%cp /content/yolov5/runs/train/yolov5s_results/weights/best.pt /content/yolov5/weights"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HmbxbKGgeu_7"
      },
      "source": [
        "### Log Performance and Training"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "btiKlApce2Gi"
      },
      "source": [
        "I trained this model for chunk of 100 epochs at a time, and saved over the previous best.pt file for use as the starting weights of the next chunk each time.\n",
        "\n",
        "1. 0-99: 32% MAP @ 0.5\n",
        "2. 100-199: 68% MAP @ 0.5\n",
        "3. 200-299: 84% MAP @ 0.5\n",
        "4. 300-399: 89% MAP @ 0.5\n",
        "\n",
        "Once I had reached 400 epochs, the MAP started to level off, so I ran one final chunk of 100 epochs, this time with a batch size of 8 instead of 16.\n",
        "\n",
        "5. 400-499: 92% MAP @ 0.5\n",
        "\n",
        "Then, I slowed down to chunks of 50, because the batch size of 8 slowed down the it/s. Performance improvement slowed down a lot after 500 epochs.\n",
        "6. 500-549: 94% MAP @ 0.5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vxxqTl5Lr0xq"
      },
      "source": [
        "### Run Inference Model "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LgyxW9n8r6n0"
      },
      "source": [
        "# use the best weights!\n",
        "!python detect.py --weights /content/yolov5/weights/best.pt --img 416 --conf 0.2 --source /content/yolov5/American-Sign-Language-Letters-1/test/images --name /content/yolov5/runs/detect/output --exist-ok"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qv40JgCq5WRI"
      },
      "source": [
        "## (20 pts) Task 3 Justify how well your model performs\n",
        "Provide charts showing at least the Mean Average Precision, Accuracy, and Recall of your model. Discuss the charts and what these results mean. You may want to include a discussion on overfitting and underfitting in your discussion. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6n4JUSlKMVye"
      },
      "source": [
        "### Visualize starting Data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mnsBeBGtMXJ1"
      },
      "source": [
        "# first, display our ground truth data\n",
        "print(\"GROUND TRUTH TRAINING DATA:\")\n",
        "Image(filename='/content/yolov5/runs/train/yolov5s_results/test_batch0_labels.jpg', width=900)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "esgdNCmXMX85"
      },
      "source": [
        "### Visualize Results"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gF-c0tiJMZNw"
      },
      "source": [
        "#display inference on ALL test images\n",
        "#this looks much better with longer training above\n",
        "for imageName in glob.glob('/content/yolov5/runs/detect/output/*.jpg'): #assuming JPG\n",
        "    display(Image(filename=imageName))\n",
        "    print(\"\\n\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "a0kyfGGCq5S5"
      },
      "source": [
        "### Evaluate the Model with Tensorboard"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dBi5Qy_n5m3L"
      },
      "source": [
        "# Start tensorboard\n",
        "# Launch after you have started training\n",
        "# logs save in the folder \"runs\"\n",
        "%load_ext tensorboard\n",
        "%tensorboard --logdir runs"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EzJlZA7v5sR3"
      },
      "source": [
        "### Answer to Task 3. Add any discussion of your models to this box. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ys-bw08N5xVj"
      },
      "source": [
        "## (20 pts) Task 4. Connect your model to Webcam streaming\n",
        "\n",
        "Enable your model to predict in real-time on a web camera, using your custom model.\n",
        "Hint: On the left hand side, Google Colab gives you code for accessing your webcam with Javascript. Use this code to get your webcam working. This will not be the same as real-time webcam footage, but it will get you as close as you can get on Google Colab. To execute this step\n",
        "\n",
        "1.   Modify the webcam code so that it will open the webcam,\n",
        "2.   take a picture,\n",
        "1.   close the webcam,\n",
        "2.   run the detection algorithm,\n",
        "1.   display the detection image\n",
        "2.   and then repeat 5 times.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9PK0ROFlZHB"
      },
      "source": [
        "### Create the take_photo() method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GKihEaYKOnKT"
      },
      "source": [
        "def take_photo(filename='webcamPhoto.jpg', quality=0.8):\n",
        "  js = Javascript('''\n",
        "    async function takePhoto(quality) {\n",
        "      const div = document.createElement('div');\n",
        "      const capture = document.createElement('button');\n",
        "      capture.textContent = 'Capture';\n",
        "      div.appendChild(capture);\n",
        "\n",
        "      const video = document.createElement('video');\n",
        "      video.style.display = 'block';\n",
        "      const stream = await navigator.mediaDevices.getUserMedia({video: true});\n",
        "\n",
        "      document.body.appendChild(div);\n",
        "      div.appendChild(video);\n",
        "      video.srcObject = stream;\n",
        "      await video.play();\n",
        "\n",
        "      // Resize the output to fit the video element.\n",
        "      google.colab.output.setIframeHeight(document.documentElement.scrollHeight, true);\n",
        "\n",
        "      // Wait for Capture to be clicked.\n",
        "      await new Promise((resolve) => capture.onclick = resolve);\n",
        "\n",
        "      const canvas = document.createElement('canvas');\n",
        "      canvas.width = video.videoWidth;\n",
        "      canvas.height = video.videoHeight;\n",
        "      canvas.getContext('2d').drawImage(video, 0, 0);\n",
        "      stream.getVideoTracks()[0].stop();\n",
        "      div.remove();\n",
        "      return canvas.toDataURL('image/jpeg', quality);\n",
        "    }\n",
        "    ''')\n",
        "  display(js)\n",
        "  data = eval_js('takePhoto({})'.format(quality))\n",
        "  binary = b64decode(data.split(',')[1])\n",
        "  with open(filename, 'wb') as f:\n",
        "    f.write(binary)\n",
        "  return filename"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qi73CeU_leOd"
      },
      "source": [
        "### Take 5 webcam photos and define them"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mkoS9U_HOnKX"
      },
      "source": [
        "for i in range(5):\n",
        "  print(\"Image \" + str(i))\n",
        "  filename = take_photo()\n",
        "  # run the inference model on the image\n",
        "  !python detect.py --weights /content/yolov5/weights/best.pt --img 416 --conf 0.2 --source webcamPhoto.jpg --name /content/yolov5/runs/detect/webcam --exist-ok\n",
        "  # print the detection image\n",
        "  display(Image(filename='/content/yolov5/runs/detect/webcam/webcamPhoto.jpg'))\n",
        "  print(\"\\n\") "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cfzjxmhy7jbI"
      },
      "source": [
        "## (10 pts) Task 5. Create something interesting using your model\n",
        "\n",
        "You have a limited time, so don't make this a huge feature, just something cool your model could do. For example, something that counts the objects coming across the webcam would receive full points. If you have a more creative idea, I may be inclined to add some bonus points, but make sure you are taking care of your other classwork as well. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WQ29zSM4vF8U"
      },
      "source": [
        "### For my Task 5, I chose to write the beginning of an ASL translator, that takes images being shown to the webcam and outputs a translation in text. Note that ASL is usual spoken one word at a time, not by letters, so more usuable versions would need to be trained on a complete ASL dictionary."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jrBJ-WAG2lW2"
      },
      "source": [
        "###Define a number to letter converter function to write class number as the letter it represents"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2x7RweeV2p9t"
      },
      "source": [
        "def numberToLetter(numberInput):\n",
        "  number = int(numberInput)\n",
        "  if number is 0:\n",
        "    return \"a\"\n",
        "  if number is 1:\n",
        "    return \"b\"\n",
        "  if number is 2:\n",
        "    return \"c\"\n",
        "  if number is 3:\n",
        "    return \"d\"\n",
        "  if number is 4:\n",
        "    return \"e\"\n",
        "  if number is 5:\n",
        "    return \"f\"\n",
        "  if number is 6:\n",
        "    return \"g\"\n",
        "  if number is 7:\n",
        "    return \"h\"\n",
        "  if number is 8:\n",
        "    return \"i\"\n",
        "  if number is 9:\n",
        "    return \"j\"\n",
        "  if number is 10:\n",
        "    return \"k\"\n",
        "  if number is 11:\n",
        "    return \"l\"\n",
        "  if number is 12:\n",
        "    return \"m\"\n",
        "  if number is 13:\n",
        "    return \"n\"\n",
        "  if number is 14:\n",
        "    return \"o\"\n",
        "  if number is 15:\n",
        "    return \"p\"\n",
        "  if number is 16:\n",
        "    return \"q\"\n",
        "  if number is 17:\n",
        "    return \"r\"\n",
        "  if number is 18:\n",
        "    return \"s\"\n",
        "  if number is 19:\n",
        "    return \"t\"\n",
        "  if number is 20:\n",
        "    return \"u\"\n",
        "  if number is 21:\n",
        "    return \"v\"\n",
        "  if number is 22:\n",
        "    return \"w\"\n",
        "  if number is 23:\n",
        "    return \"x\"\n",
        "  if number is 24:\n",
        "    return \"y\"\n",
        "  if number is 25:\n",
        "    return \"z\"\n",
        "  else:\n",
        "    return \"?\""
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQQk2iI1l32I"
      },
      "source": [
        "### Print output to get the user started, capture one sign at a time and print text detections"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bOjjcpC5poLw"
      },
      "source": [
        "print(\"click the capture button to take a picture of each letter. The translation will be output below.\\n\")\n",
        "\n",
        "done = \"n\"\n",
        "word = \"Word: \"\n",
        "\n",
        "while done is not \"y\":\n",
        "  filename = take_photo()\n",
        "  !python detect.py --weights /content/yolov5/weights/best.pt --img 416 --conf 0.2 --source webcamPhoto.jpg --project /content/yolov5/runs/detect/translation --exist-ok --save-txt\n",
        "  path = Path('/content/yolov5/runs/detect/translation/exp/labels/webcamPhoto.txt')\n",
        "  if path.is_file() and path.stat().st_size > 0:\n",
        "    with open(path, \"r+\") as f:\n",
        "      number = f.readline().split()[0]\n",
        "      f.truncate(0)\n",
        "    word = word + numberToLetter(number)\n",
        "    print(word)\n",
        "  done = input(\"done? (y/n)\")\n",
        "\n",
        "print(\"Collection finished. \" + word)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KvelYJcI8Ove"
      },
      "source": [
        "## (10 pts) Task 6. Write 300-500 words on the ethical implications of your project\n",
        "Make sure you support your thoughts with ethical frameworks from ACM, IEEE, or any other reputible source. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ktf121Zv8c8c"
      },
      "source": [
        ">There are very interesting ethical considerations with all Machine Learning and Artificial Intelligence applications, but I think that translation brings up some very interesting concerns. At the surface level, this is a very moral application of computer science, because it is using computer vision to help people who communicate in different ways understand each other. This is in line with the ACM code of ethics principle 1.1, “Contribute to society and to human well-being”.  Additionally, helping different kinds of people get equal opportunities relates to principle 1.4, “Be fair and take action not to discriminate”. Therefore, the overall mission of vision-to-text and vision-to-speech translation is based on morally sound principles of contributing to society and helping others.\n",
        "\n",
        ">However, there are many places where this goal could potentially go wrong. One potential misstep would be to record use of the translator to create new test cases for online learning. This could very easily be seen as a good idea because it creates more training data to help continually improve the application and make it better for users. If individuals to do not volunteer this data for use in training, or do not have any way of using the application without being required to have their use recorded, this is a clear violation of principles 1.6 and 1.7, “respect privacy”, and “honor confidentiality”.\n",
        "\n",
        ">Additionally, consider the general practice in machine learning that models are never left solely in charge of making a classification or decision in a situation; there is always a human somewhere behind the model verifying its decisions. For the same reasons, adding another human monitoring someone’s conversations through the translator would be a breach of privacy. Yet, eliminating this verification layer means that it is only likely that the translator makes a mistake (especially because ASL is an extremely contextual language) and puts words into someone’s mouth, which is a violation of principle 2.1 “Strive to achieve high quality in both the processes and products of professional work” because a translator that does not output what the person is saying is objectively low quality work.\n",
        "\n",
        ">Ultimately, given these considerations I believe that the most ethically sound application for this application would be to assist or replace an ASL interpreter on TV, where the content being communicated is publicized and therefore there exists no reasonable expectation of privacy. \n"
      ]
    }
  ]
}